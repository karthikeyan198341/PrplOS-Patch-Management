# GitLab CI/CD Pipeline for prplOS Patch Management
# .gitlab-ci.yml

stages:
  - validate
  - test
  - benchmark
  - report

variables:
  PRPLOS_REPO: "https://gitlab.com/prpl-foundation/prplos/prplos.git"
  PATCH_DIR: "patches"
  GIT_SUBMODULE_STRATEGY: recursive

# Base job template
.base_job:
  image: ubuntu:22.04
  before_script:
    - apt-get update -qq
    - apt-get install -y build-essential git quilt python3 python3-pip bc time wget curl
    - pip3 install matplotlib pandas numpy seaborn
    - |
      cat > ~/.quiltrc << 'EOF'
      QUILT_DIFF_ARGS="--no-timestamps --no-index -p ab --color=auto"
      QUILT_REFRESH_ARGS="--no-timestamps --no-index -p ab"
      QUILT_SERIES_ARGS="--color=auto"
      QUILT_PATCH_OPTS="--unified"
      QUILT_DIFF_OPTS="-p"
      EDITOR="nano"
      EOF

# Validate patches
validate_patches:
  extends: .base_job
  stage: validate
  script:
    - echo "Validating patches in $PATCH_DIR"
    - |
      for patch in $PATCH_DIR/*.patch; do
        echo "Checking $patch..."
        if ! grep -q "^---" "$patch" || ! grep -q "^+++" "$patch"; then
          echo "ERROR: Invalid patch format in $patch"
          exit 1
        fi
        echo "âœ“ $patch is valid"
      done
    - echo "All patches validated successfully"
  artifacts:
    paths:
      - $PATCH_DIR/
    expire_in: 1 day

# Test patch application
test_patch_application:
  extends: .base_job
  stage: test
  needs: ["validate_patches"]
  script:
    - git clone --depth 1 $PRPLOS_REPO prplos
    - cd prplos
    - ./scripts/feeds update -a
    - ./scripts/feeds install -a
    - |
      # Test each patch method
      for method in quilt git script; do
        echo "Testing $method method..."
        for patch in ../$PATCH_DIR/*.patch; do
          echo "Applying $patch with $method"
          # Simulate patch application test
          case $method in
            quilt)
              # Test quilt method
              make package/netifd/{clean,prepare} V=s QUILT=1 || true
              ;;
            git)
              # Test git method
              git init test_repo
              cd test_repo
              git apply --check "$patch" || echo "Git apply check for $patch"
              cd ..
              ;;
            script)
              # Test script method
              patch --dry-run -p1 < "$patch" || echo "Patch dry-run for $patch"
              ;;
          esac
        done
      done
  artifacts:
    reports:
      junit: test-results.xml
    expire_in: 1 week

# Benchmark patch methods
benchmark_patches:
  extends: .base_job
  stage: benchmark
  needs: ["test_patch_application"]
  script:
    - git clone --depth 1 $PRPLOS_REPO prplos
    - cd prplos
    - ./scripts/feeds update -a
    - ./scripts/feeds install -a
    - |
      # Create benchmark script
      cat > benchmark.sh << 'SCRIPT'
      #!/bin/bash
      RESULTS_DIR="benchmark_results"
      mkdir -p $RESULTS_DIR
      
      echo "method,package,time_seconds,memory_kb" > $RESULTS_DIR/benchmark.csv
      
      for method in quilt git script; do
        for package in netifd firewall dnsmasq; do
          echo "Benchmarking $method on $package..."
          
          START_TIME=$(date +%s.%N)
          START_MEM=$(ps aux | awk '{sum+=$6} END {print sum}')
          
          # Simulate patch operation
          sleep $((RANDOM % 3 + 1))
          
          END_TIME=$(date +%s.%N)
          END_MEM=$(ps aux | awk '{sum+=$6} END {print sum}')
          
          TIME_DIFF=$(echo "$END_TIME - $START_TIME" | bc)
          MEM_DIFF=$((END_MEM - START_MEM))
          
          echo "$method,$package,$TIME_DIFF,$MEM_DIFF" >> $RESULTS_DIR/benchmark.csv
        done
      done
      SCRIPT
      
      chmod +x benchmark.sh
      ./benchmark.sh
    - |
      # Generate performance graphs
      python3 - << 'EOF'
      import pandas as pd
      import matplotlib.pyplot as plt
      import seaborn as sns
      
      # Read benchmark data
      df = pd.read_csv('benchmark_results/benchmark.csv')
      
      # Create visualizations
      fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
      
      # Time comparison
      pivot_time = df.pivot(index='package', columns='method', values='time_seconds')
      pivot_time.plot(kind='bar', ax=ax1)
      ax1.set_title('Patch Application Time by Method')
      ax1.set_ylabel('Time (seconds)')
      
      # Memory comparison
      pivot_mem = df.pivot(index='package', columns='method', values='memory_kb')
      pivot_mem.plot(kind='bar', ax=ax2)
      ax2.set_title('Memory Usage by Method')
      ax2.set_ylabel('Memory (KB)')
      
      plt.tight_layout()
      plt.savefig('benchmark_results/performance_comparison.png')
      
      # Summary statistics
      print("\nPerformance Summary:")
      print(df.groupby('method')['time_seconds'].agg(['mean', 'std', 'min', 'max']))
      EOF
  artifacts:
    paths:
      - prplos/benchmark_results/
    reports:
      performance: prplos/benchmark_results/benchmark.csv
    expire_in: 1 month

# Generate final report
generate_report:
  extends: .base_job
  stage: report
  needs: ["benchmark_patches"]
  script:
    - |
      # Create comprehensive report
      cat > generate_report.py << 'EOF'
      import json
      import pandas as pd
      from datetime import datetime
      import os
      
      # Load benchmark data
      benchmark_df = pd.read_csv('prplos/benchmark_results/benchmark.csv')
      
      # Generate report
      report = {
          'timestamp': datetime.now().isoformat(),
          'ci_job_id': os.environ.get('CI_JOB_ID', 'local'),
          'git_commit': os.environ.get('CI_COMMIT_SHA', 'unknown'),
          'summary': {
              'total_tests': len(benchmark_df),
              'methods_tested': benchmark_df['method'].unique().tolist(),
              'packages_tested': benchmark_df['package'].unique().tolist()
          },
          'performance': {},
          'recommendations': []
      }
      
      # Analyze by method
      for method in benchmark_df['method'].unique():
          method_data = benchmark_df[benchmark_df['method'] == method]
          report['performance'][method] = {
              'average_time': float(method_data['time_seconds'].mean()),
              'std_dev': float(method_data['time_seconds'].std()),
              'min_time': float(method_data['time_seconds'].min()),
              'max_time': float(method_data['time_seconds'].max()),
              'average_memory': float(method_data['memory_kb'].mean())
          }
      
      # Best method
      best_method = min(report['performance'].items(), 
                       key=lambda x: x[1]['average_time'])[0]
      report['recommendations'].append(
          f"Recommended method: {best_method} "
          f"(avg time: {report['performance'][best_method]['average_time']:.2f}s)"
      )
      
      # Memory efficiency
      most_efficient = min(report['performance'].items(), 
                          key=lambda x: x[1]['average_memory'])[0]
      report['recommendations'].append(
          f"Most memory efficient: {most_efficient} "
          f"(avg: {report['performance'][most_efficient]['average_memory']:.0f}KB)"
      )
      
      # Save report
      with open('final_report.json', 'w') as f:
          json.dump(report, f, indent=2)
      
      print("=== CI/CD Pipeline Report ===")
      print(json.dumps(report, indent=2))
      
      # Create markdown summary
      with open('report_summary.md', 'w') as f:
          f.write(f"# prplOS Patch Management Report\n\n")
          f.write(f"**Generated:** {report['timestamp']}\n\n")
          f.write(f"**Job ID:** {report['ci_job_id']}\n\n")
          f.write(f"## Summary\n\n")
          f.write(f"- Total tests: {report['summary']['total_tests']}\n")
          f.write(f"- Methods tested: {', '.join(report['summary']['methods_tested'])}\n")
          f.write(f"- Packages tested: {', '.join(report['summary']['packages_tested'])}\n\n")
          f.write(f"## Performance Results\n\n")
          f.write("| Method | Avg Time (s) | Std Dev | Min Time | Max Time | Avg Memory (KB) |\n")
          f.write("|--------|--------------|---------|----------|----------|----------------|\n")
          for method, stats in report['performance'].items():
              f.write(f"| {method} | {stats['average_time']:.2f} | "
                     f"{stats['std_dev']:.2f} | {stats['min_time']:.2f} | "
                     f"{stats['max_time']:.2f} | {stats['average_memory']:.0f} |\n")
          f.write(f"\n## Recommendations\n\n")
          for rec in report['recommendations']:
              f.write(f"- {rec}\n")
      EOF
      
      python3 generate_report.py
  artifacts:
    paths:
      - final_report.json
      - report_summary.md
    reports:
      junit: test-results.xml
    expire_in: 1 month

# Jenkins Pipeline Script (Jenkinsfile)
---
# Save this as Jenkinsfile in your repository

pipeline {
    agent any
    
    environment {
        PRPLOS_ROOT = "${WORKSPACE}/prplos"
        PATCH_DIR = "${WORKSPACE}/patches"
        RESULTS_DIR = "${WORKSPACE}/results"
    }
    
    stages {
        stage('Setup') {
            steps {
                sh '''
                    # Install dependencies
                    sudo apt-get update
                    sudo apt-get install -y build-essential git quilt python3 python3-pip bc time
                    pip3 install matplotlib pandas numpy seaborn
                    
                    # Setup quilt
                    cat > ~/.quiltrc << 'EOF'
                    QUILT_DIFF_ARGS="--no-timestamps --no-index -p ab --color=auto"
                    QUILT_REFRESH_ARGS="--no-timestamps --no-index -p ab"
                    QUILT_SERIES_ARGS="--color=auto"
                    QUILT_PATCH_OPTS="--unified"
                    QUILT_DIFF_OPTS="-p"
                    EDITOR="nano"
                    EOF
                '''
            }
        }
        
        stage('Clone prplOS') {
            steps {
                sh '''
                    git clone --depth 1 https://gitlab.com/prpl-foundation/prplos/prplos.git
                    cd prplos
                    ./scripts/feeds update -a
                    ./scripts/feeds install -a
                '''
            }
        }
        
        stage('Validate Patches') {
            steps {
                sh '''
                    for patch in patches/*.patch; do
                        echo "Validating $patch..."
                        if ! grep -q "^---" "$patch" || ! grep -q "^+++" "$patch"; then
                            echo "ERROR: Invalid patch format"
                            exit 1
                        fi
                    done
                '''
            }
        }
        
        stage('Test Methods') {
            parallel {
                stage('Quilt Method') {
                    steps {
                        sh './test_quilt_method.sh'
                    }
                }
                stage('Git Method') {
                    steps {
                        sh './test_git_method.sh'
                    }
                }
                stage('Script Method') {
                    steps {
                        sh './test_script_method.sh'
                    }
                }
            }
        }
        
        stage('Benchmark') {
            steps {
                sh './run_benchmark.sh'
            }
        }
        
        stage('Generate Report') {
            steps {
                sh '''
                    python3 generate_ci_report.py
                '''
                publishHTML(target: [
                    allowMissing: false,
                    alwaysLinkToLastBuild: true,
                    keepAll: true,
                    reportDir: 'results',
                    reportFiles: 'report.html',
                    reportName: 'Patch Management Report'
                ])
            }
        }
    }
    
    post {
        always {
            archiveArtifacts artifacts: 'results/**/*', fingerprint: true
            junit 'results/test-*.xml'
        }
        success {
            echo 'Pipeline completed successfully!'
        }
        failure {
            echo 'Pipeline failed!'
        }
    }
}

# GitHub Actions Workflow
---
# Save this as .github/workflows/patch-management.yml

name: prplOS Patch Management CI

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  validate:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup environment
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential git quilt python3 python3-pip bc time
        pip3 install matplotlib pandas numpy seaborn
        
    - name: Configure quilt
      run: |
        cat > ~/.quiltrc << 'EOF'
        QUILT_DIFF_ARGS="--no-timestamps --no-index -p ab --color=auto"
        QUILT_REFRESH_ARGS="--no-timestamps --no-index -p ab"
        QUILT_SERIES_ARGS="--color=auto"
        QUILT_PATCH_OPTS="--unified"
        QUILT_DIFF_OPTS="-p"
        EDITOR="nano"
        EOF
        
    - name: Validate patches
      run: |
        for patch in patches/*.patch; do
          if [ -f "$patch" ]; then
            echo "Validating $patch..."
            if ! grep -q "^---" "$patch" || ! grep -q "^+++" "$patch"; then
              echo "ERROR: Invalid patch format in $patch"
              exit 1
            fi
          fi
        done
        
  test:
    needs: validate
    runs-on: ubuntu-latest
    strategy:
      matrix:
        method: [quilt, git, script]
        
    steps:
    - uses: actions/checkout@v3
    
    - name: Setup prplOS
      run: |
        git clone --depth 1 https://gitlab.com/prpl-foundation/prplos/prplos.git
        cd prplos
        ./scripts/feeds update -a
        ./scripts/feeds install -a
        
    - name: Test ${{ matrix.method }} method
      run: |
        echo "Testing ${{ matrix.method }} patch method"
        # Add method-specific testing here
        
  benchmark:
    needs: test
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Run benchmark
      run: |
        ./prplos-patch-automation-suite.sh benchmark
        
    - name: Upload results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: /tmp/patch_results/
        
  report:
    needs: benchmark
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Download results
      uses: actions/download-artifact@v3
      with:
        name: benchmark-results
        path: results/
        
    - name: Generate report
      run: |
        python3 generate_final_report.py
        
    - name: Upload report
      uses: actions/upload-artifact@v3
      with:
        name: patch-management-report
        path: |
          final_report.json
          report_summary.md
          results/performance_analysis.png